\chapter{Microscopic Roughness}
\thispagestyle{empty}% no page number in chapter title page
girizgah
\section{Methods for Interpolating Audio Signals}
Interpolate audio signals for different velocities.

There are methods: lpc and major frequency.

\subsection{Linear Predictive Coding}
The basic idea of Linear Predictive Coding (LPC) is to develop a transfer function that can predict each sample of a signal as a linear combination of the previous samples. It has applications in filter design and speech coding. 

We consider an IIR filter $H(z)$ of length n in the form $H(z)=[-h_1z^{-2}-h_2z^{-1}...-h_nz^{-n}]$.
Our acceleration data vector from PCA is called {$\vec{a}(k)$} in the following. The resulting prediction vector from our filter is $\vec{\hat{a}}(k)$. The residual signal {$\vec{e}(k)$} is the difference between these two signals.The transfer function $P$(z) is the result of the following equation:

\begin{equation} \label{eq:transferfunction}
\frac{\vec{e}(k)}{\vec{a}(k)}=1-H(z)=P(z)   
\end{equation}

It is possible to compute the residual at each step using the vector of filter coefficients $\vec{h}=[h_1 h_2 h_3 ... h_n]^T$:

\begin{equation} \label{eq:residual}
\vec{e}(k)=a(k)-\hat{a}(k)=a(k)-\vec{h}^T\vec{a}(k-1)   
\end{equation}

At this step, we aim to find the minimum value of the residual function $e(k)$. We are able to reduce the problem to Wiener-Hopf equation by a cost function based on mean-square error. The Wiener-Hopf equation can be solved by Levinson-Durbin \cite{durbin} algorithm, so that we can obtain our optimal filter vector $\vec{h_0}$. 

To synthesize new signals, we use a white noise signal {$\vec{e_g}(l)$} as input, which is filtered with $1/P(z)$, in order to generate our desired response {$\vec{a_g}(l)$}. For a better overview, we can rewrite the equations (\ref{eq:transferfunction}) and (\ref{eq:residual}) as follows:

\begin{equation}
\frac{\vec{a_g}(l)}{\vec{e_g}(l)}=\frac{1}{1-H(z)} =\frac{1}{P(z)} 
\end{equation}

\begin{equation}
a_g(l)=e_g(l)+\vec{h}^T\vec{a_g}(l-1) 
\end{equation}

The value {$\vec{e_g}(l)$} is a randomly generated Gaussian white noise but its average signal power must be equal to that of the average signal power remaining in the residual, $P\{\vec{e}(k)\}$ after filter optimization.

The definition of power is as in the following equation:

\begin{equation}
P\{\vec{a}(l)\}=\frac{1}{N}\sum_{n=0}^{N-1}|a(n)|^2
\end{equation}

This is equivalent to signal variance $\sigma²$ ,because our signals are zero-mean signals. Now, we have to determine the order of our prediction filter, which affect the accuracy of the prediction. The higher we choose the order, the smaller the residual gets. It means we have a better prediction with higher orders, but then the calculation gets  more complicated. It is possible to calculate the success of the synthetic result with a cost function defined as the RMS error as follows \cite{romano10}:

\begin{equation} \label{costfunction}
C\{\vec{a_g}(l)\}=\frac{RMS(DFT_s\{\vec{a}(l)\}-DFT_s\{\vec{\hat{{a_g}}}(l)\})}{RMS(DFT_s\{\vec{a}(l)\}}
\end{equation}

Using this equation, where $DFT_s\{\vec{a}\}$ represents the discrete Fourier transform of vector $\vec{a}$, it is possible to obtain the optimal order of the filter. In our case we choose $length(\vec{a}(l))-1$ as the order for the best quality of results.

Now that we have generated our prediction filter with two unique variables $\vec{h}$ vector and $e_g(l)$, it comes to interpolate between our synthesized signals to create new signals. Bilinear interpolation of both the vector $\vec{h}$ and $e_g(l)$ of two signals in different velocities and applying these new values to our prediction filter  result in new synthesized signals, so that we create signal data for audio signals at different force and velocities.

\section{Beispiel für eine Abbildung}
\begin{figure}[h!]
  \begin{center}
        \includegraphics[width=4cm]{TUMLogo_oZ_Outline_schwarz_CMYK}
    \caption{Beispiel für eine Beschriftung.}
    \label{fig:ToUseWithReference}
  \end{center}
\end{figure}

\begin{equation} \label{eq:isim}
mRG = {\beta \cdot \sum_{k=1}^K \sum_{l=1}^L \hat{\textbf{X}}(k,l)}
\end{equation}

Durch die \texttt{\bslash label} kann auf die Bilder mit
\texttt{\bslash ref} verwiesen werden
(z.B.~Abbildung~\ref{fig:ToUseWithReference}).

\section{Beispiele für Referenzen}
Die Literaturhinweise werden im Text z.B.\ folgendermaßen verwendet:\\
``..., wie in \cite{eberspaecher97} gezeigt, ...'' oder ``... es gibt
mehrere Ansätze \cite{arnaud99,griswold90} ...''

\section{Schrifttypen}
Als Schrifttyp wird Arial oder Roman empfohlen. Bitte beachten, daß
Größen und Einheiten eine eigene Schreibweise haben:
\begin{description}
\item[Kursivschrift:] physikalische Größen (z.B.~$U$ für Spannung),
  Variablen~(z.B.~$x$), sowie Funktions- und Operatorzeichen, deren
  Bedeutung frei gewählt werden kann (z.B.~$f(x)$)
\item[Steilschrift:] Einheiten und ihre Vorsätze (z.B.~kg, pF),
  Zahlen, Funktions- und Operatorzeichen mit feststehender Bedeutung
  (z.B.~sin, lg)
\end{description} 

\clearpage

\section{Archivierung}
Für die Archivierung sind alle Dateien der Arbeit (auch der Vorträge)
dem Betreuer zur Verfügung zu stellen.  Weiterhin soll noch ein
\BibTeX-Eintrag der Arbeit erstellt werden (die Felder in eckigen
Klammern sind dabei auszufüllen):
\begin{verbatim}
@MastersThesis{<Nachname des Autors><Jahr>,
  type =         {<Art der Arbeit>},
  title =        {{<Thema der Arbeit>}},
  school =       {Institute of Communication Networks~(LKN),
                  Munich University of Technology~(TUM)},
  author =       {<Nachname des Autors>, <Vorname des Autors>},
  annote =       {<Nachname des Betreuers>, <Vorname des Betreuers>},
  month =        {<Monat>},
  year =         {<Jahr>},
  key =          {<Mehrere Suchschlüssel>}
}
\end{verbatim}
